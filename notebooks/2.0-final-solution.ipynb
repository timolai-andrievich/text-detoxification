{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final solution is a GPT-2 model finetuned to the detoxifying task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "torch.random.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_URL = \"https://github.com/skoltech-nlp/detox/\" +\\\n",
    "    \"releases/download/emnlp2021/filtered_paranmt.zip\"\n",
    "with open('filtered.zip', 'wb') as file, requests.get(DOWNLOAD_URL, stream=True) as req:\n",
    "    file.write(req.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('filtered.zip') as zfile, open('filtered.tsv', 'wb') as file:\n",
    "    zfile.extract('filtered.tsv')\n",
    "os.remove('filtered.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('filtered.tsv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicDataset(Dataset):\n",
    "    \"\"\"A simple dataset class which stores reference-translation pairs concatenated together.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe: pd.DataFrame):\n",
    "        \"\"\"A simple dataset class which stores reference-translation pairs\n",
    "        concatenated together.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): The dataset dataframe. Must contain\n",
    "            columnts: `reference`, `translation`, `ref_tox`, `trn_tox`.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        separate_token = '<|endoftext|>'\n",
    "        eos_token = '<|endoftext|>'\n",
    "        for _, (ref, trn, ref_tox, trn_tox) in dataframe[['reference', 'translation', 'ref_tox', 'trn_tox']].iterrows():\n",
    "            if ref_tox < trn_tox:\n",
    "                ref, trn = trn, ref\n",
    "            self.data.append(f'{ref}{separate_token}{trn}{eos_token}')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ToxicDataset(dataframe)\n",
    "train_size, test_size = len(dataset) - 100, 100\n",
    "train_ds, test_ds = random_split(dataset, (train_size, test_size))\n",
    "train_loader = DataLoader(train_ds, batch_size=1)\n",
    "test_loader = DataLoader(test_ds, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import transformers\n",
    "from torch.utils import tensorboard as torchboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on \"cuda\"\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Training on \"{device}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = 9500\n",
    "warmup_steps = 500\n",
    "max_step = warmup_steps + training_steps\n",
    "learning_rate = 3e-5\n",
    "epochs = 1\n",
    "log_dir = './runs'\n",
    "save_dir = './gpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\eleven\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained('gpt2').to(device)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2')\n",
    "optimizer = transformers.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, warmup_steps, training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    step = 0\n",
    "    writer = torchboard.writer.SummaryWriter(log_dir)\n",
    "    pbar = tqdm.notebook.tqdm(total=max_step, desc='Training')\n",
    "    for epoch in range(epochs):\n",
    "        for texts in train_loader:\n",
    "            text, = texts\n",
    "            if step > max_step:\n",
    "                break\n",
    "            tokens = tokenizer.encode(text)\n",
    "            inputs = torch.tensor(tokens).to(device)\n",
    "            loss = model(inputs, labels=inputs)['loss']\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            scalars = {\n",
    "                'Loss': loss.item(),\n",
    "                'Step': step,\n",
    "                'Epoch': epoch,\n",
    "            }\n",
    "            for metric, value in scalars.items():\n",
    "                writer.add_scalar(metric, value, global_step=step)\n",
    "            writer.flush()\n",
    "            step += 1\n",
    "            pbar.update()\n",
    "            pbar.set_postfix(scalars)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training can be skipped if there is a pretrained model available.\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained('./gpt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8345b6f2f6f4c178ddc8088849cd640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\eleven\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detoxify(text: str, max_len=100) -> str:\n",
    "    \"\"\"Uses the model to detoxify a small text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to be detoxified.\n",
    "        max_len (int, optional): Maximum length of the translated\n",
    "        text in tokens. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        str: De-toxified text. Capitalization may be incorrect.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text) + [tokenizer.eos_token_id]\n",
    "    initial_len = len(tokens)\n",
    "    for _ in range(max_len):\n",
    "        tokens_tensor = torch.tensor(tokens).to(device)\n",
    "        logits = model(tokens_tensor)['logits']\n",
    "        next_token = logits[-1, :].argmax()\n",
    "        if next_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "        tokens.append(next_token)\n",
    "    detoxified_tokens = tokens[initial_len:]\n",
    "    return tokenizer.decode(detoxified_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval() -> float:\n",
    "    \"\"\"Evaluates the model and returns the BLEU score.\n",
    "\n",
    "    Returns:\n",
    "        float: BLEU score, ranging from 0 to 1.\n",
    "    \"\"\"\n",
    "    pbar = tqdm.notebook.tqdm(total=len(test_loader), desc='Evaluation')\n",
    "    bleu = torchmetrics.text.BLEUScore()\n",
    "    scores = []\n",
    "    logits = []\n",
    "    for texts in test_loader:\n",
    "        text, = texts\n",
    "        ref, ground_trn, _ = text.split('<|endoftext|>')\n",
    "        trn = detoxify(text)\n",
    "        pbar.update(1)\n",
    "        score = bleu([trn], [[ground_trn]]).item()\n",
    "        scores.append(score)\n",
    "    pbar.close()\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4027c7e77d9a430a88e73fe68d2f4438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bleu = eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 27\n"
     ]
    }
   ],
   "source": [
    "print(f'BLEU score: {bleu * 100:.0f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('eleven')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28c38045a217c3494aecda864b402de2332328bd6799c470faa4b728b2109968"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
